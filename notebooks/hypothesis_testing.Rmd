---
title: "Hypothesis testing on MovieLens dataset"
date: 21/07/2018
author: José Benardi de Souza Nunes
output:
  html_notebook:
    toc: yes
    toc_float: yes
  html_document:
    df_print: paged
    toc: yes
    toc_float: yes
---

<br></br>

# Introduction

</br>

> This report is an analysis on the dataset movielens which can be found in full [here](https://grouplens.org/datasets/movielens/latest/). The code, data, a description of the variables used in this report and another report employing Confindence Intervals on the same dataset can be found in the [original repository](https://github.com/Benardi/movielens_analysis/)

</br>

***

</br>

```{r setup, echo=FALSE, warning=FALSE, message=FALSE}
library(here)
library(coin)
library(resample)
library(tidyverse)


theme_set(theme_bw())
```

# Data Overview 

</br>

## Loading and filtering data

```{r, warning=FALSE}
readr::read_csv(here::here("data/movies.csv"),
                progress = FALSE,
                col_types = cols(
                      movieId = col_integer(),
                      title = col_character(),
                      genres = col_character()
                    )) %>% 
  group_by(movieId) %>%
  mutate(year = as.numeric(sub("\\).*", "",sub(".*\\(", "", title))),
         num_genres = length(as.list(strsplit(genres,'|',fixed = TRUE))[[1]]),
         homogeneous = num_genres == 1, # Deriving homogeneity
         xx_century = year <= 2000
         ) %>%  
  na.omit() %>%
  ungroup() -> movies

readr::read_csv(here::here("data/ratings.csv"),
                progress = FALSE,
                col_types = cols(
                userId = col_integer(),
                movieId = col_integer(),
                rating = col_double(),
                timestamp = col_integer()
                )) %>%
  na.omit() -> ratings
```

```{r}
dplyr::inner_join(
  movies,
  ratings,
  by="movieId") -> data


data %>%
  group_by(movieId) %>%
  summarise(median_rating = median(rating), # Deriving whether a movies is well rated
            well_rated = median_rating > 3.5) -> summarised


dplyr::inner_join(
  summarised,
  data,
  by="movieId") -> data

data %>%
  glimpse()
```

```{r}
movies %>%
  filter(title == "Hamlet (2000)")

data %>%
  filter(movieId != 3598) -> data
```

* Looks like Hamlet was included twice. As Hamlet is clearly a Drama we're gonna remove the entries that say otherwise.

<br>

## Checking overall data consistency

```{r}
data %>%
  group_by(movieId) %>%
  slice(1) %>%
  ggplot(aes(year)) +
  geom_bar() +
  labs(x="Movie Year",
       y="Absolute Frequency")
```

* There's a whole lot of movies from the 2000's.
* There's movies going as far as the first decade of the 1900's.

```{r}
data %>%
  ggplot(aes(num_genres)) +
  geom_bar() +
  labs(x="Number of Genres",
       y="Absolute Frequency")
```

* Most movies have 2 or 3 genres.

```{r}
data %>%
  ggplot(aes(rating,y=..prop..)) +
  geom_bar() +
  labs(x="Movie Rating", 
       y="Relative Frequency")
```

* There's a lot of generous reviews, with 4 stars composing almost 30% of the ratings. 

<br>

## Conclusion

<br>

* Anomalous data filtered (i.e. Hamlet).

* No further data found violating expected range or categories.

<br>

***

<br>

# Hypothesis testing considerations

<br>

#### Null hypothesis

There's an association in our sample (an estimate that can be observed), and we want to test whether this association in the sample is significant (it in fact exists) or is mere result of chance.  
To do so we use as reference a situation where there's no association, we call this version of the sample the _null hypothesis_. 

<br>

#### Permutation test

In the case of the *permutation test* the association in the sample is nullified by applying a random process on it, and thus we generate a  _null hypothesis_.

<br>

#### Hypothesis test

* Firstly we observe an estimate in the data (An association).
    + If an estimate such as the one we observed in the sample happens easily in the _null hypothesis_ (P-Values above alpha, see below) it means that we don' have strong evidence of association. What we observed happens when there's no association as well.
    + If an estimate such as the one we observed is improbable in the _null hypothesis_ (P-Values below alpha, see below) we have an indicative of association. If an association as the one we observed in the sample (estimate) doesn't happen when there's no association, then there's evidence of an association (**Double negative**). 
    
<br>
    
#### P-Value

**P-Value** is the probability of the _null hypothesis_ rendering an association such as the one we observed in the sample. 

>The more times the _null hypothesis_ succeds at creating estimates as extreme as the ones we observed in the sample the higher is the chance that the estimate/association we observed in the sample is merely result of chance, because chance ( _null hypothesis_ ) is doing the same as our sample.   

We can phrase the **P-Value** in two ways:

* When we look at the _null hypothesis_ **P-Value** is the proportion of the estimates produced by the _null hypothesis_ that are as extreme as the one we observed in the sample.
* **P-Value** is the proportion of the behavior of the _null hypothesis_ where it succeeds at repeating the association we observed in the sample.

<br>

#### Significance Level (Alpha)

The significance level, also denoted as alpha or α, is the probability of rejecting the null hypothesis when it is true. For example, a significance level of 0.05 indicates a 5% risk of concluding that a difference exists when there is no actual difference.

A more intuitive way to understand alpha may be that the smaller the alpha the more extreme the values the _null hypothesis_ has to create for we to consider similar to the one we observed in the sample. 

* It's something of a culture around the community that uses Statistics to use a Alpha of 0.05 ( 5% risk of concluding that a difference exists when there is no actual difference).
* While this somewhat magic number should be taken with a grain of salt, it's still widely accepted outside tests that impact the safety of people (e.g. drug tests). 

> We will use a Significance Level (Alpha) of 0.05 throughout this analysis.

<br>

# Proportion of well rated movies 

<br>

#### Is there a difference in the proportion of well rated movies when we compare movies from XX and XXI century?

<br>

* Movies are considered well rated when they have been rated above 2.5 stars in terms of median. 
* We will use as estimator the unpaired difference of the proportion of well rated (TRUE - FALSE):
    + **The proportion of well rated of movies from the XX century minus the proportion of well rated of movies from the XXI century**.
    
<br>

### Let's use the resample package

```{r}
permutationTest2(data = data,
                 R=5000,
                 statistic = prop.table(table(well_rated))[2],
                 treatment = xx_century)
```

<br>

* In accordance to our previous results employing a **Confindence Interval** our results using a _two tailed permutation test_ do suggest that there's in fact an association and consequently the null hypothesis should be rejected.
    +  The _two tailed permutation test_ rendered a **PValue** (0.00039992) far below the chosen **alpha** (0.05) which indicates that an association is indeed present, therefore at a confidence level of 95% we can say that there's a statistically significant difference between the proportion of well rated movies of movies from the XX century and movies from the XXI century.    

<br>

### Let's use the coin package

```{r}
independence_test(well_rated ~ xx_century, 
                  data = data)
```

<br>

* Once again we have results that agree with our previous results employing a **Confindence Interval**.
    +  The _two tailed permutation test_ rendered a **PValue** (2.2e-16) far below the chosen **alpha** (0.05) which indicates that an association is indeed present, therefore at a confidence level of 95% we can say that there's a statistically significant difference between the proportion of well rated movies of movies from the XX century and movies from the XXI century.  

# C.I vs Hypothesis Testing

<br>

As said at the introduction of this report, this very data-set was analyzed using _confindence intervals_ as well. So now that both tools have been used remains the question? What's the difference?

Both C.I. and Hypothesis Testing may help us weight whether:

* A is significantly different from B
    + (A != B)
* A is significantly bigger than B
    + (A > B)
* A is significantly smaller than B
    + (A < B)

However when we employ Hypothesis Testing we have no practical implication of the magnitude of the finding, in other words we can't possibly say how much bigger A is than B, or how much smaller A is than B.  

> With Hypothesis Testing you can pretty much only discuss whether the effect is present or not, while with confidence intervals we can also talk about the magnitude of the effect.



